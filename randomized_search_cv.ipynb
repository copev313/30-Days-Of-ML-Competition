{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 30 Days of Machine Learning Competition "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "## Step 1: Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# For encoding categorical variables & splitting data:\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\r\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\r\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\r\n",
    "\r\n",
    "# For models:\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from xgboost import XGBRegressor\r\n",
    "\r\n",
    "# For scoring:\r\n",
    "from sklearn.metrics import mean_squared_error\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "## Step 2: Load the Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load the training data:\r\n",
    "training_df = pd.read_csv(\"data/train.csv\", index_col=0)\r\n",
    "testing_df = pd.read_csv(\"data/test.csv\", index_col=0)\r\n",
    "\r\n",
    "# Preview the data:\r\n",
    "print(training_df.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9  ...     cont5     cont6  \\\n",
      "id                                                    ...                       \n",
      "1     B    B    B    C    B    B    A    E    C    N  ...  0.400361  0.160266   \n",
      "2     B    B    A    A    B    D    A    F    A    O  ...  0.533087  0.558922   \n",
      "3     A    A    A    C    B    D    A    D    A    F  ...  0.650609  0.375348   \n",
      "4     B    B    A    C    B    D    A    E    C    K  ...  0.668980  0.239061   \n",
      "6     A    A    A    C    B    D    A    E    A    N  ...  0.686964  0.420667   \n",
      "\n",
      "       cont7     cont8     cont9    cont10    cont11    cont12    cont13  \\\n",
      "id                                                                         \n",
      "1   0.310921  0.389470  0.267559  0.237281  0.377873  0.322401  0.869850   \n",
      "2   0.516294  0.594928  0.341439  0.906013  0.921701  0.261975  0.465083   \n",
      "3   0.902567  0.555205  0.843531  0.748809  0.620126  0.541474  0.763846   \n",
      "4   0.732948  0.679618  0.574844  0.346010  0.714610  0.540150  0.280682   \n",
      "6   0.648182  0.684501  0.956692  1.000773  0.776742  0.625849  0.250823   \n",
      "\n",
      "      target  \n",
      "id            \n",
      "1   8.113634  \n",
      "2   8.481233  \n",
      "3   8.364351  \n",
      "4   8.049253  \n",
      "6   7.972260  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "### Seperating the Data from the Target"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Seperate the target variable from the features:\r\n",
    "y = training_df['target']\r\n",
    "features = training_df.drop(['target'], axis=1)\r\n",
    "\r\n",
    "# Preview features:\r\n",
    "print(features.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9  ...     cont4     cont5  \\\n",
      "id                                                    ...                       \n",
      "1     B    B    B    C    B    B    A    E    C    N  ...  0.610706  0.400361   \n",
      "2     B    B    A    A    B    D    A    F    A    O  ...  0.276853  0.533087   \n",
      "3     A    A    A    C    B    D    A    D    A    F  ...  0.285074  0.650609   \n",
      "4     B    B    A    C    B    D    A    E    C    K  ...  0.284667  0.668980   \n",
      "6     A    A    A    C    B    D    A    E    A    N  ...  0.287595  0.686964   \n",
      "\n",
      "       cont6     cont7     cont8     cont9    cont10    cont11    cont12  \\\n",
      "id                                                                         \n",
      "1   0.160266  0.310921  0.389470  0.267559  0.237281  0.377873  0.322401   \n",
      "2   0.558922  0.516294  0.594928  0.341439  0.906013  0.921701  0.261975   \n",
      "3   0.375348  0.902567  0.555205  0.843531  0.748809  0.620126  0.541474   \n",
      "4   0.239061  0.732948  0.679618  0.574844  0.346010  0.714610  0.540150   \n",
      "6   0.420667  0.648182  0.684501  0.956692  1.000773  0.776742  0.625849   \n",
      "\n",
      "      cont13  \n",
      "id            \n",
      "1   0.869850  \n",
      "2   0.465083  \n",
      "3   0.763846  \n",
      "4   0.280682  \n",
      "6   0.250823  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "## Step 3: Prepare the Data\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# List of categorical columns:\r\n",
    "category_cols = [col for col in features.columns if 'cat' in col]\r\n",
    "\r\n",
    "# Remove any categorical columns:\r\n",
    "#category_cols.remove('cat2')\r\n",
    "#category_cols.remove('cat4')\r\n",
    "#category_cols.remove('cat6')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# List of numerical columns:\r\n",
    "number_cols = [col for col in features.columns if 'cat' not in col]\r\n",
    "\r\n",
    "# Remove any columns here:\r\n",
    "#number_cols = number_cols.remove('col_name')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Preprocessing Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Preprocessing for categorical columns:\r\n",
    "cat_transformer = Pipeline(\r\n",
    "    steps=[\r\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\r\n",
    "        ('ordinal', OrdinalEncoder()),\r\n",
    "    ]\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "# Preprocessing for numerical columns:\r\n",
    "num_transformer = Pipeline(\r\n",
    "    steps=[\r\n",
    "        ('simple', SimpleImputer(strategy='constant')),\r\n",
    "    ]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bundle Preprocessing Steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Bundle preprocessing into column tranformer:\r\n",
    "preprocessor = ColumnTransformer(\r\n",
    "    transformers = [\r\n",
    "        ('cat', cat_transformer, category_cols),\r\n",
    "        ('num', num_transformer, number_cols),\r\n",
    "    ]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a Copy of Our DataFrames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X = features.copy()\r\n",
    "X_test = testing_df.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split the Data Into a Training & Validation Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Split data:\r\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.3, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "## Step 4: Setting Up & Training the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Number of trees in random forest:\r\n",
    "n_estimators = [int(x) for x in list(range(100, 401, 50))]\r\n",
    "\r\n",
    "# Number of features to consider at every split:\r\n",
    "max_features = ['auto', 'sqrt']\r\n",
    "\r\n",
    "# Maximum number of levels in tree:\r\n",
    "#max_depth = [int(x) for x in list(range(10, 61, 5))]\r\n",
    "#max_depth.append(None)\r\n",
    "\r\n",
    "# Minimum number of samples required to split a node:\r\n",
    "min_samples_split = [2, 5, 10]\r\n",
    "\r\n",
    "# Minimum number of samples required at each leaf node:\r\n",
    "min_samples_leaf = [1, 2, 4, 8]\r\n",
    "\r\n",
    "# Method of selecting samples for training each tree:\r\n",
    "bootstrap = [True, False]\r\n",
    "\r\n",
    "# Create the random grid:\r\n",
    "random_grid = {\r\n",
    "    'n_estimators': n_estimators,\r\n",
    "    'max_features': max_features,\r\n",
    "    #'max_depth': max_depth,\r\n",
    "    'min_samples_split': min_samples_split,\r\n",
    "    'min_samples_leaf': min_samples_leaf,\r\n",
    "    'bootstrap': bootstrap\r\n",
    "}\r\n",
    "\r\n",
    "from functools import reduce\r\n",
    "print(random_grid)\r\n",
    "print(reduce(lambda x, y: x*y, [len(x) for x in random_grid.values()]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'n_estimators': [100, 150, 200, 250, 300, 350, 400], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4, 8], 'bootstrap': [True, False]}\n",
      "336\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Use the random grid to search for best hyperparameters\r\n",
    "# First create the base model to tune\r\n",
    "rf = RandomForestRegressor()\r\n",
    "\r\n",
    "# Random search of parameters, using 3 fold cross validation, \r\n",
    "# search across 100 different combinations, and use all available cores\r\n",
    "rf_random = RandomizedSearchCV(\r\n",
    "    estimator=rf, \r\n",
    "    param_distributions=random_grid,\r\n",
    "    n_iter=100,\r\n",
    "    scoring='neg_root_mean_squared_error',\r\n",
    "    cv=3,\r\n",
    "    verbose=2,\r\n",
    "    random_state=0,\r\n",
    "    n_jobs=-1\r\n",
    ")\r\n",
    "\r\n",
    "X_processed = preprocessor.fit_transform(X_train)\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15012/1427393174.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Use the random grid to search for best hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# First create the base model to tune\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Random search of parameters, using 3 fold cross validation,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Fit the random search model\r\n",
    "rf_random.fit(X_processed, y_train)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View the best params from fitting the random search:\r\n",
    "print(rf_random.best_params_)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evalute the random search model:\r\n",
    "def evaluate(model, test_features, test_labels):\r\n",
    "    predictions = model.predict(test_features)\r\n",
    "    errors = abs(predictions - test_labels)\r\n",
    "    mape = 100 * np.mean(errors / test_labels)\r\n",
    "    accuracy = 100 - mape\r\n",
    "    print('Model Performance')\r\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\r\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\r\n",
    "    \r\n",
    "    return accuracy\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Define the model + parameters:\r\n",
    "model = RandomForestRegressor(n_estimators=115,\r\n",
    "                              random_state=0,\r\n",
    "                              n_jobs=-1)\r\n",
    "\r\n",
    "\r\n",
    "#model = XGBRegressor(n_estimators=350,\r\n",
    "#                     learning_rate=0.05,\r\n",
    "#                     n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Define a main pipeline:\r\n",
    "my_pipeline = Pipeline(\r\n",
    "    steps = [\r\n",
    "        ('preprocessor', preprocessor),\r\n",
    "        ('model', model),\r\n",
    "    ]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Our Model Using the Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Fit the model:\r\n",
    "my_pipeline.fit(X_train, y_train)\r\n",
    "\r\n",
    "\r\n",
    "'''\r\n",
    "# Preformat\r\n",
    "preprocessor.fit(X_valid)\r\n",
    "X_valid_transformed = preprocessor.transform(X_valid)\r\n",
    "\r\n",
    "\r\n",
    "my_pipeline.fit(X_train, y_train,\r\n",
    "                model__early_stopping_rounds=20,\r\n",
    "                model__eval_set=[(X_valid_transformed, y_valid)],\r\n",
    "                model__verbose=False\r\n",
    "               )\r\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n# Preformat\\npreprocessor.fit(X_valid)\\nX_valid_transformed = preprocessor.transform(X_valid)\\n\\n\\nmy_pipeline.fit(X_train, y_train,\\n                model__early_stopping_rounds=20,\\n                model__eval_set=[(X_valid_transformed, y_valid)],\\n                model__verbose=False\\n               )\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "## Step 5: Evaluating Our Model\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make A Prediction On the Validation Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Generate prediction about the validation dataset:\r\n",
    "pred_validate = my_pipeline.predict(X_validate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Score Our Predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Score using mean squared error (minus the squared for the competition):\r\n",
    "mse = mean_squared_error(y_validate, pred_validate, squared=False)\r\n",
    "\r\n",
    "print(\"MSE: \", round(mse, 7))  # 0.737501"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE:  0.7375536\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Determine Feature Importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Create a DataFrame to display feature importances:\r\n",
    "feature_importances = pd.DataFrame({\r\n",
    "    'features': category_cols + number_cols,\r\n",
    "    'importance': model.feature_importances_ * 100,\r\n",
    "})\r\n",
    "\r\n",
    "print(feature_importances)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   features  importance\n",
      "0      cat0    0.652925\n",
      "1      cat1    0.506290\n",
      "2      cat2    0.475970\n",
      "3      cat3    0.619199\n",
      "4      cat5    0.846176\n",
      "5      cat7    0.516848\n",
      "6      cat8    1.449139\n",
      "7      cat9    3.063522\n",
      "8     cont0    6.345342\n",
      "9     cont1    6.215426\n",
      "10    cont2    6.819642\n",
      "11    cont3    6.452880\n",
      "12    cont4    6.469812\n",
      "13    cont5    6.606862\n",
      "14    cont6    6.237868\n",
      "15    cont7    6.540410\n",
      "16    cont8    6.211484\n",
      "17    cont9    6.666669\n",
      "18   cont10    7.216699\n",
      "19   cont11    6.368271\n",
      "20   cont12    7.247675\n",
      "21   cont13    6.470889\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implement Cross Validation For Better Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "kfold = KFold(shuffle=True, n_splits=4)\r\n",
    "\r\n",
    "cv_scores = -1 * cross_val_score(my_pipeline, X, y,\r\n",
    "                                 cv=kfold,\r\n",
    "                                 n_jobs=-1,\r\n",
    "                                 scoring='neg_root_mean_squared_error')\r\n",
    "print(cv_scores)\r\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nkfold = KFold(shuffle=True, n_splits=4)\\n\\ncv_scores = -1 * cross_val_score(my_pipeline, X, y,\\n                                 cv=kfold,\\n                                 n_jobs=-1,\\n                                 scoring='neg_root_mean_squared_error')\\nprint(cv_scores)\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "## Step 6: Create a Submission File"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Use the model to make predictions:\r\n",
    "predictions = my_pipeline.predict(X_test)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the predictions to a CSV file:\r\n",
    "output = pd.DataFrame({\r\n",
    "    'Id': X_test.index,\r\n",
    "    'target': predictions,\r\n",
    "})\r\n",
    "\r\n",
    "output.to_csv(\"submission.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "## Step 7: Alternative Approaches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're not sure what to do next, you can begin by trying out more model types!\r\n",
    "1. If you took the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course, then you learned about **[XGBoost](https://www.kaggle.com/alexisbcook/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\r\n",
    "​\r\n",
    "2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https://www.kaggle.com/svyatoslavsokolov/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "0111817ea0767685089815a8346b7ddb44dc6686759b000d5a9694abd8ceb5ac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}